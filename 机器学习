1.k-means k均值聚类
  算法思想：
  1.初始化k个向量
  2.根据样本数据距离的向量为依据和一个向量最近的样本划为一类，如此划分子集
  3.用从属于某一类的样本均值取代该向量
  4.如上进行迭代，直到运行到某一个轮数，或者向量改变小于阈值
  def k_means(D,k):
          while(True)
          for j in range(1,k+1)
                Cj=空集
                Uj=随机向量
           for i in range(1,n+1)
                for j in range(1,k+1)
                #求每个样本和每个向量之间的距离并找到最小距离
                    dij=dist(xi,uj)
                 min_di =min(dij for j in range(1,k+2))
                 #假设 min_di 对应的最小距离类为p
                 Cp =CP  U xi
                 
  


2.学习向量化是一个监督学习的算法，但他的思想跟一般的聚类算法比较相似也就是说，样本本身带有标记信息,
算法的工作就是为每一组类别的变量找到一个代表向量。
  算法思想如下：
  1。随机初始化k个表示向量，并设定他们分别为第1....k类
  2.随机选择一个样本，寻找离他最近的表示向量
  3.更新该表示向量--如果表示向量所属类别和样本相同，就靠近该样本，否则远离该样本
  4.重复2-4 步骤，直到达最大轮数位置
  # 输入样本 D={(x1,y1)......(xm,ym)},类别总数目k，训练轮数T
 def LVQ(D,k,T):
  初始化k个表示向量p1,p2....pk
     # 训练T轮
    for t in range(1,T+1):
        随机挑选一个样本(xr,yr)
         寻找最近的表示向量ps
        # 更新表示向量
        refresh(ps)
     return {p1,....pk}
     

3层次聚类
1.将m个样本看作m个已经划分好的子集
2.找出距离最近的2个聚类子集，将他们合并
3.重复步骤2，直到剩余k个子集。
  计算距离，一般有3种表示：
   最小距离：将两个集合中距离最近的两个元素的距离当作集合的距离
   最大距离：将两个集中中距离最远的两个元素的距离当作集合的距离
   平均距离：
   豪斯多夫距离
